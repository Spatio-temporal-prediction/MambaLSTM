{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.计算网格中心点\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def calculate_grid_centers(input_file_path, output_file_path):\n",
    "    \"\"\"\n",
    "    计算网格中心点坐标的函数\n",
    "    参数:\n",
    "        input_file_path: 输入CSV文件路径\n",
    "        output_file_path: 输出CSV文件路径\n",
    "    \"\"\"\n",
    "    # 读取输入文件\n",
    "    df = pd.read_csv(input_file_path)\n",
    "    \n",
    "    # 提取经纬度范围\n",
    "    longitude_ranges = df['Longitude Range'].apply(lambda x: tuple(map(float, x[1:-1].split(', '))))\n",
    "    latitude_ranges = df['Latitude Range'].apply(lambda x: tuple(map(float, x[1:-1].split(', '))))\n",
    "    \n",
    "    # 计算中心点坐标\n",
    "    df['Center Longitude'] = (longitude_ranges.apply(lambda x: x[0]) + longitude_ranges.apply(lambda x: x[1])) / 2\n",
    "    df['Center Latitude'] = (latitude_ranges.apply(lambda x: x[0]) + latitude_ranges.apply(lambda x: x[1])) / 2\n",
    "    \n",
    "    # 保存中心点坐标\n",
    "    df[['Center Latitude', 'Center Longitude']].to_csv(output_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate_grid_centers(r'save/NYC_grid_c.csv', 'save/grid_centers_c.csv')\n",
    "# calculate_grid_centers(r'save/NYC_grid_f.csv', 'save/grid_centers_f.csv')\n",
    "# calculate_grid_centers(r'save/NYC_grid_uf.csv', 'save/grid_centers_uf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算flow值（即出租车行程数据，乘客的人流量）\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import box\n",
    "import itertools\n",
    "\n",
    "def calculate_grid_flows(grid_file_path, taxi_zones_file_path, taxi_trips_file_path, start_date, end_date, output_file_path):\n",
    "    \"\"\"\n",
    "    计算网格的流入量和流出量\n",
    "    参数:\n",
    "        grid_file_path: 细粒度网格CSV文件路径\n",
    "        taxi_zones_file_path: 出租车区GeoJSON文件路径\n",
    "        taxi_trips_file_path: 出租车出行CSV文件路径\n",
    "        start_date: 起始日期\n",
    "        end_date: 结束日期\n",
    "        output_file_path: 输出CSV文件路径\n",
    "    \"\"\"\n",
    "    # 读取细粒度网格数据\n",
    "    grid_data = pd.read_csv(grid_file_path)\n",
    "    print(\"read1\")\n",
    "\n",
    "    # 读取出租车区数据\n",
    "    taxi_zones = gpd.read_file(taxi_zones_file_path)\n",
    "    print(\"read1\")\n",
    "\n",
    "    # 读取出租车出行数据\n",
    "    taxi_trips = pd.read_csv(taxi_trips_file_path)\n",
    "    print(\"read1\")\n",
    "\n",
    "    # 将出租车出行数据按时间段进行分组\n",
    "    taxi_trips['pickup_datetime'] = pd.to_datetime(taxi_trips['tpep_pickup_datetime'], format='%Y-%m-%d %H:%M:%S')\n",
    "    taxi_trips['dropoff_datetime'] = pd.to_datetime(taxi_trips['tpep_dropoff_datetime'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    # 过滤出指定日期范围内的出租车出行数据\n",
    "    start_date = pd.to_datetime(start_date)\n",
    "    end_date = pd.to_datetime(end_date)\n",
    "    taxi_trips = taxi_trips[(taxi_trips['pickup_datetime'] >= start_date) & (taxi_trips['pickup_datetime'] <= end_date)]\n",
    "\n",
    "    # 过滤出起点和终点不在同一个出租车区的行程\n",
    "    taxi_trips = taxi_trips[taxi_trips['PULocationID'] != taxi_trips['DOLocationID']]\n",
    "\n",
    "    # 计算时间段\n",
    "    taxi_trips['time_slot'] = ((taxi_trips['pickup_datetime'] - start_date).dt.total_seconds() // 3600).astype(int)\n",
    "\n",
    "    # 将网格数据转换为 GeoDataFrame,并指定坐标参考系统为 EPSG:4326\n",
    "    grid_data['geometry'] = grid_data.apply(lambda row: box(eval(row['Longitude Range'])[0], eval(row['Latitude Range'])[0], eval(row['Longitude Range'])[1], eval(row['Latitude Range'])[1]), axis=1)\n",
    "    grid_data = gpd.GeoDataFrame(grid_data, geometry='geometry', crs='EPSG:4326')\n",
    "\n",
    "    # 将 grid_data 的坐标参考系统转换为与 taxi_zones 一致\n",
    "    grid_data = grid_data.to_crs(taxi_zones.crs)\n",
    "\n",
    "    # 计算所有网格和出租车区的重叠部分\n",
    "    overlaps = gpd.sjoin(grid_data, taxi_zones, predicate='intersects')\n",
    "\n",
    "    # 创建包含所有网格 ID 和时间段组合的空 DataFrame,并将 inflow 和 outflow 列全部填充为 0\n",
    "    grid_ids = grid_data['grid_id'].unique()\n",
    "    time_slots = range(int((pd.to_datetime(end_date) - pd.to_datetime(start_date)).total_seconds() // 3600))\n",
    "    grid_flow_df = pd.DataFrame(list(itertools.product(grid_ids, time_slots)), columns=['grid_id', 'time_slot'])\n",
    "    grid_flow_df['inflow'] = 0.0\n",
    "    grid_flow_df['outflow'] = 0.0\n",
    "    print(\"begin\")\n",
    "\n",
    "    # 遍历每个网格\n",
    "    for grid_id, group in overlaps.groupby('grid_id'):\n",
    "        \n",
    "        print(f\"Processing grid {grid_id}\")\n",
    "        if grid_id <50:\n",
    "            continue\n",
    "        print(\"Overlapping taxi zones:\", end=' ')\n",
    "        print(group['objectid'].astype(int).tolist())\n",
    "\n",
    "        # 筛选出与当前网格相关的出租车出行数据\n",
    "        relevant_trips = taxi_trips[(taxi_trips['PULocationID'].isin(group['objectid'].astype(int))) | (taxi_trips['DOLocationID'].isin(group['objectid'].astype(int)))]\n",
    "\n",
    "        # 计算每个出租车区的流入量和流出量,并按时间段和出租车区 ID 进行分组\n",
    "        zone_inflow = relevant_trips.groupby(['time_slot', 'DOLocationID'])['passenger_count'].sum()\n",
    "        zone_outflow = relevant_trips.groupby(['time_slot', 'PULocationID'])['passenger_count'].sum()\n",
    "\n",
    "        # 将流入量和流出量按重叠面积的比例累加到对应的网格和时间段上\n",
    "        for (time_slot, zone_id), inflow_count in zone_inflow.items():\n",
    "            overlap_rows = overlaps.loc[(overlaps['grid_id'] == int(grid_id)) & (overlaps['objectid'].astype(int) == zone_id), 'geometry']\n",
    "            if len(overlap_rows) > 0:\n",
    "                overlap_rows_proj = overlap_rows.to_crs(epsg=3857)\n",
    "                taxi_zones_proj = taxi_zones.to_crs(epsg=3857)\n",
    "                overlap_ratio = overlap_rows_proj.area.values[0] / taxi_zones_proj.loc[taxi_zones_proj['objectid'].astype(int) == zone_id, 'geometry'].area.values[0]\n",
    "                grid_flow_df.loc[(grid_flow_df['grid_id'] == int(grid_id)) & (grid_flow_df['time_slot'] == time_slot), 'inflow'] += inflow_count * overlap_ratio\n",
    "\n",
    "        for (time_slot, zone_id), outflow_count in zone_outflow.items():\n",
    "            overlap_rows = overlaps.loc[(overlaps['grid_id'] == int(grid_id)) & (overlaps['objectid'].astype(int) == zone_id), 'geometry']\n",
    "            if len(overlap_rows) > 0:\n",
    "                overlap_rows_proj = overlap_rows.to_crs(epsg=3857)\n",
    "                taxi_zones_proj = taxi_zones.to_crs(epsg=3857)\n",
    "                overlap_ratio = overlap_rows_proj.area.values[0] / taxi_zones_proj.loc[taxi_zones_proj['objectid'].astype(int) == zone_id, 'geometry'].area.values[0]\n",
    "                grid_flow_df.loc[(grid_flow_df['grid_id'] == int(grid_id)) & (grid_flow_df['time_slot'] == time_slot), 'outflow'] += outflow_count * overlap_ratio\n",
    "\n",
    "        output_file=\"save/\"+output_file_path+str(grid_id)+\".csv\"\n",
    "        grid_flow_df.to_csv(output_file, index=False)        \n",
    "    # grid_flow_df.to_csv(output_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read1\n",
      "read1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read1\n",
      "begin\n",
      "Processing grid 0\n",
      "Processing grid 3\n",
      "Processing grid 4\n",
      "Processing grid 5\n",
      "Processing grid 6\n",
      "Processing grid 7\n",
      "Processing grid 8\n",
      "Processing grid 9\n",
      "Processing grid 10\n",
      "Processing grid 11\n",
      "Processing grid 12\n",
      "Processing grid 13\n",
      "Processing grid 14\n",
      "Processing grid 15\n",
      "Processing grid 16\n",
      "Processing grid 17\n",
      "Processing grid 18\n",
      "Processing grid 19\n",
      "Processing grid 20\n",
      "Processing grid 21\n",
      "Processing grid 22\n",
      "Processing grid 23\n",
      "Processing grid 24\n",
      "Processing grid 25\n",
      "Processing grid 26\n",
      "Processing grid 27\n",
      "Processing grid 28\n",
      "Processing grid 29\n",
      "Processing grid 30\n",
      "Processing grid 31\n",
      "Processing grid 32\n",
      "Processing grid 33\n",
      "Processing grid 34\n",
      "Processing grid 35\n",
      "Processing grid 36\n",
      "Processing grid 37\n",
      "Processing grid 38\n",
      "Processing grid 39\n",
      "Processing grid 40\n",
      "Processing grid 41\n",
      "Processing grid 42\n",
      "Processing grid 43\n",
      "Processing grid 44\n",
      "Processing grid 45\n",
      "Processing grid 46\n",
      "Processing grid 47\n",
      "Processing grid 48\n",
      "Processing grid 49\n",
      "Processing grid 50\n",
      "Overlapping taxi zones: [34, 217, 256, 80, 232, 4, 224, 137, 255, 112, 145, 233, 37, 36, 198, 157, 226, 170]\n",
      "Processing grid 51\n",
      "Overlapping taxi zones: [226, 260, 7, 207, 179, 82, 83, 173, 129, 70, 138, 53, 199, 223, 8, 194]\n",
      "Processing grid 52\n",
      "Overlapping taxi zones: [145, 233, 229, 202, 193, 140, 141, 237, 226, 146, 7, 179, 223, 8, 194, 170, 162, 161, 163, 142, 239, 238, 262, 263, 236, 43, 75]\n",
      "Processing grid 53\n",
      "Overlapping taxi zones: [93, 56, 173, 57, 192, 73, 253, 70, 138, 53, 92, 171, 252]\n",
      "Processing grid 54\n",
      "Overlapping taxi zones: [199, 126, 213, 212, 60, 223, 194, 168, 159, 147, 167, 247, 69, 248, 47]\n",
      "Processing grid 55\n",
      "Overlapping taxi zones: [194, 168, 159, 247, 69, 119, 238, 43, 75, 151, 24, 41, 74, 166, 152, 42, 116, 244, 120]\n",
      "Processing grid 56\n",
      "Overlapping taxi zones: [53, 213, 212, 250, 252, 208]\n",
      "Processing grid 57\n",
      "Overlapping taxi zones: [212, 250, 208, 58, 183, 248, 182, 242, 185, 32, 3, 184, 51, 81, 254]\n",
      "Processing grid 58\n",
      "Overlapping taxi zones: [46, 184]\n",
      "Processing grid 59\n",
      "Overlapping taxi zones: [184, 81]\n",
      "Processing grid 60\n",
      "Overlapping taxi zones: [212, 60, 167, 247, 69, 119, 248, 78, 242, 185, 20, 31, 32, 18, 59, 47, 120, 169, 235, 94, 127, 136, 128, 174, 254, 153, 241, 220]\n",
      "Processing grid 61\n",
      "Overlapping taxi zones: [247, 119, 42, 116, 244, 120, 235, 243, 127, 128]\n",
      "Processing grid 62\n",
      "Overlapping taxi zones: [3, 184, 51, 81, 254, 240, 259]\n",
      "Processing grid 63\n",
      "Overlapping taxi zones: [31, 18, 128, 174, 254, 240, 259, 153, 241, 220, 200]\n",
      "Processing grid 64\n",
      "Overlapping taxi zones: [128, 220]\n",
      "细网格计算完成\n"
     ]
    }
   ],
   "source": [
    "calculate_grid_flows(\n",
    "    grid_file_path=r'save/NYC_grid_c.csv', \n",
    "    taxi_zones_file_path=r\"dataset/nyc_taxi_zones.geojson\", \n",
    "    taxi_trips_file_path=r\"dataset/yellow_tripdata_2013_all.csv\", \n",
    "    start_date='2013-01-01', \n",
    "    end_date='2013-12-31', \n",
    "    output_file_path='grid_flow_c/'\n",
    ")\n",
    "print('细网格计算完成')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate_grid_flows(\n",
    "#     grid_file_path=r'save/NYC_grid_f.csv', \n",
    "#     taxi_zones_file_path=r\"dataset/nyc_taxi_zones.geojson\", \n",
    "#     taxi_trips_file_path=r\"dataset/yellow_tripdata_2013_all.csv\", \n",
    "#     start_date='2013-01-01', \n",
    "#     end_date='2014-01-01', \n",
    "#     output_file_path='grid_flow_f/'\n",
    "# )\n",
    "# print('细网格计算完成')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate_grid_flows(\n",
    "#     grid_file_path=r'save/NYC_grid_uf.csv', \n",
    "#     taxi_zones_file_path=r\"dataset/nyc_taxi_zones.geojson\", \n",
    "#     taxi_trips_file_path=r\"dataset/yellow_tripdata_2013_all.csv\", \n",
    "#     start_date='2013-01-01', \n",
    "#     end_date='2014-01-01', \n",
    "#     output_file_path='grid_flow_uf/'\n",
    "# )\n",
    "# print('极细网格计算完成')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 文件与对应有效区间的映射\n",
    "# file_ranges = {\n",
    "#     '0-88': (0, 88),\n",
    "#     '89-91': (89, 91),\n",
    "#     '92-95': (60, 74),\n",
    "#     '96-107': (75, 89),\n",
    "#     '108-109': (90, 103),\n",
    "#     '110-111': (104, 137),\n",
    "#     '112-114': (138, 139),\n",
    "#     '115-117': (140, 144),\n",
    "#     '118-119': (145, 146),\n",
    "#     '120-214': (147, 153),\n",
    "#     '215-217': (154, 199)\n",
    "# }\n",
    "file_ranges = {\n",
    "    '0-32': (0, 32),\n",
    "    '33-42': (33, 42),\n",
    "    '43-49': (43, 49),\n",
    "    '50-64': (50, 64)\n",
    "}\n",
    "\n",
    "combined_data = []\n",
    "\n",
    "for filename, (start, end) in file_ranges.items():\n",
    "    # 读取CSV文件（假设文件扩展名为.csv）\n",
    "    filepath = f\"save/c/{filename}.csv\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    # 筛选有效区间的数据\n",
    "    valid_data = df[(df['grid_id'] >= start) & (df['grid_id'] <= end)]\n",
    "    combined_data.append(valid_data)\n",
    "\n",
    "# 合并所有数据并排序\n",
    "final_df = pd.concat(combined_data, ignore_index=True)\n",
    "final_df.sort_values(by=['grid_id', 'time_slot'], inplace=True)\n",
    "\n",
    "# 保存为完整文件\n",
    "final_df.to_csv(\"complete_grid_flow_c.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "结果已保存到 save/grid_poi_c.csv\n"
     ]
    }
   ],
   "source": [
    "# 3.计算POI值\n",
    "import pandas as pd\n",
    "\n",
    "def process_poi_data(grid_file, poi_file, output_file):\n",
    "    \"\"\"\n",
    "    处理兴趣点数据并计算每个网格中的不同类型兴趣点数量。\n",
    "    \n",
    "    参数:\n",
    "    grid_file (str): 网格数据文件路径。\n",
    "    poi_file (str): 兴趣点数据文件路径。\n",
    "    output_file (str): 输出结果文件路径。\n",
    "    \"\"\"\n",
    "    # 读取网格数据和兴趣点数据\n",
    "    grid_df = pd.read_csv(grid_file)\n",
    "    poi_df = pd.read_csv(poi_file)\n",
    "    \n",
    "    # 从 'the_geom' 中提取经纬度信息\n",
    "    poi_df[['Longitude', 'Latitude']] = poi_df['the_geom'].str.extract(r'POINT \\((-?\\d+\\.\\d+) (-?\\d+\\.\\d+)\\)')\n",
    "    poi_df['Longitude'] = poi_df['Longitude'].astype(float)\n",
    "    poi_df['Latitude'] = poi_df['Latitude'].astype(float)\n",
    "    \n",
    "    # 定义一个函数，用于判断一个点属于哪个网格\n",
    "    def get_grid_id(lon, lat, grid_df):\n",
    "        for _, row in grid_df.iterrows():\n",
    "            lon_range = eval(row['Longitude Range'])\n",
    "            lat_range = eval(row['Latitude Range'])\n",
    "            if lon_range[0] < lon <= lon_range[1] and lat_range[0] < lat <= lat_range[1]:\n",
    "                return row['grid_id']\n",
    "        return None\n",
    "\n",
    "    # 对每个兴趣点计算所属的网格ID\n",
    "    poi_df['grid_id'] = poi_df.apply(lambda row: get_grid_id(row['Longitude'], row['Latitude'], grid_df), axis=1)\n",
    "\n",
    "    # 按照网格ID和FACI_DOM进行分组，并统计每组的数量\n",
    "    result = poi_df.groupby(['grid_id', 'FACI_DOM']).size().reset_index(name='Count')\n",
    "    \n",
    "    # 使用pivot函数将FACI_DOM作为列名, Count作为对应的值\n",
    "    result = result.pivot(index='grid_id', columns='FACI_DOM', values='Count').reset_index()\n",
    "\n",
    "    # 重命名列名, 使其符合grid_id, faci_dom_1_count, faci_dom_2_count, …的格式\n",
    "    result.columns = ['grid_id'] + ['faci_dom_{}_count'.format(col) for col in result.columns[1:]]\n",
    "\n",
    "    # 创建一个全0的DataFrame，保证包含所有grid_id\n",
    "    all_grids_df = pd.DataFrame(grid_df['grid_id'].rename('grid_id'))\n",
    "\n",
    "    # 进行左连接合并\n",
    "    all_grids_df = pd.merge(all_grids_df, result, how='left', on='grid_id')\n",
    "\n",
    "    # 使用fillna函数将缺失值替换为0，并将所有值转换为整数类型\n",
    "    all_grids_df = all_grids_df.fillna(0).astype(int)\n",
    "\n",
    "    # 将结果保存到CSV文件中\n",
    "    all_grids_df.to_csv(output_file, index=False)\n",
    "    print(f\"结果已保存到 {output_file}\")\n",
    "\n",
    "# 调用函数进行处理\n",
    "# process_poi_data(\n",
    "#     grid_file='save/NYC_grid_c.csv', \n",
    "#     poi_file=r'dataset/Point_Of_Interest_1.csv', \n",
    "#     output_file='save/grid_poi_c.csv'\n",
    "# )\n",
    "\n",
    "# process_poi_data(\n",
    "#     grid_file='save/NYC_grid_f.csv', \n",
    "#     poi_file=r'dataset/Point_Of_Interest_1.csv', \n",
    "#     output_file='save/grid_poi_f.csv'\n",
    "# )\n",
    "\n",
    "process_poi_data(\n",
    "    grid_file='save/NYC_grid_c.csv', \n",
    "    poi_file=r'dataset/Point_Of_Interest_1.csv', \n",
    "    output_file='save/grid_poi_c.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6931/3233245668.py:16: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_collisions = pd.read_csv(collisions_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accidents outside of any grid:\n",
      "        CRASH DATE CRASH TIME  LONGITUDE  LATITUDE\n",
      "8       2013-07-25      18:09        NaN       NaN\n",
      "17      2013-11-25      11:04        NaN       NaN\n",
      "19      2013-11-22      17:15        NaN       NaN\n",
      "36      2013-12-21      13:30        NaN       NaN\n",
      "37      2013-12-06      15:38        NaN       NaN\n",
      "...            ...        ...        ...       ...\n",
      "203706  2013-01-07      12:51        NaN       NaN\n",
      "203709  2013-01-10      22:00        NaN       NaN\n",
      "203717  2013-01-05      17:18        NaN       NaN\n",
      "203726  2013-01-04      19:12        NaN       NaN\n",
      "203732  2013-01-10      17:52        NaN       NaN\n",
      "\n",
      "[31801 rows x 4 columns]\n",
      "171941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6931/3233245668.py:70: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  result['risk_value'].fillna(0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "结果已保存到 save/grid_risk_c.csv\n"
     ]
    }
   ],
   "source": [
    "# 4.计算Risk值\n",
    "import pandas as pd\n",
    "\n",
    "def process_collision_data(collisions_file, grids_file, start_date, end_date, output_file):\n",
    "    \"\"\"\n",
    "    处理交通事故数据并计算每个网格在指定时间段内的风险值。\n",
    "    \n",
    "    参数:\n",
    "    collisions_file (str): 交通事故数据文件路径。\n",
    "    grids_file (str): 网格数据文件路径。\n",
    "    start_date (str): 事故数据筛选的起始日期。\n",
    "    end_date (str): 事故数据筛选的结束日期。\n",
    "    output_file (str): 输出结果文件路径。\n",
    "    \"\"\"\n",
    "    # 读取交通事故数据和网格数据\n",
    "    df_collisions = pd.read_csv(collisions_file)\n",
    "    df_grids = pd.read_csv(grids_file, converters={'Longitude Range': eval, 'Latitude Range': eval})\n",
    "    \n",
    "    # 合并日期和时间列\n",
    "    df_collisions['DATETIME'] = pd.to_datetime(df_collisions['CRASH DATE'] + ' ' + df_collisions['CRASH TIME'], format='%Y-%m-%d %H:%M')\n",
    "    \n",
    "    # 筛选指定时间范围内的数据\n",
    "    df_collisions = df_collisions[(df_collisions['DATETIME'] >= start_date) & (df_collisions['DATETIME'] < end_date)]\n",
    "    \n",
    "    # 根据规则创建风险值列\n",
    "    def get_risk_value(row):\n",
    "        if row['NUMBER OF PERSONS INJURED'] == 0 and row['NUMBER OF PERSONS KILLED'] == 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return int(row['NUMBER OF PERSONS INJURED']+row['NUMBER OF PERSONS KILLED']*2+1)\n",
    "\n",
    "    df_collisions['risk_value'] = df_collisions.apply(get_risk_value, axis=1)\n",
    "\n",
    "    # 定义一个函数，用于判断事故位置属于哪个网格\n",
    "    def get_grid_id(lon, lat):\n",
    "        for _, row in df_grids.iterrows():\n",
    "            if row['Longitude Range'][0] <= lon < row['Longitude Range'][1] and row['Latitude Range'][0] <= lat < row['Latitude Range'][1]:\n",
    "                return row['grid_id']\n",
    "        return -1\n",
    "\n",
    "    # 应用函数到事故数据\n",
    "    df_collisions['grid_id'] = df_collisions.apply(lambda row: get_grid_id(row['LONGITUDE'], row['LATITUDE']), axis=1)\n",
    "\n",
    "    # 打印事故位置不在任何网格内的事故记录\n",
    "    print(\"Accidents outside of any grid:\")\n",
    "    print(df_collisions[df_collisions['grid_id'] == -1][['CRASH DATE', 'CRASH TIME', 'LONGITUDE', 'LATITUDE']])\n",
    "\n",
    "    # 删除网格ID为-1的行(表示事故位置不在任何网格内)\n",
    "    df_collisions = df_collisions[df_collisions['grid_id'] != -1]\n",
    "    print(len(df_collisions))\n",
    "\n",
    "    # 计算时间段号\n",
    "    start_date = pd.to_datetime(start_date)\n",
    "    end_date = pd.to_datetime(end_date)\n",
    "    total_hours = (end_date - start_date).total_seconds() / 3600\n",
    "    df_collisions['time_slot'] = ((df_collisions['DATETIME'] - start_date).dt.total_seconds() / 3600).astype(int)\n",
    "\n",
    "    # 创建包含所有网格ID和时间段号组合的数据框架\n",
    "    grid_ids = df_grids['grid_id'].unique()\n",
    "    time_slots = range(int(total_hours))\n",
    "    df_all_combinations = pd.DataFrame([(g, t) for g in grid_ids for t in time_slots], columns=['grid_id', 'time_slot'])\n",
    "\n",
    "    # 按网格ID和时间段号分组, 计算风险值总和\n",
    "    risk_by_grid_and_time_slot = df_collisions.groupby(['grid_id', 'time_slot'])['risk_value'].sum().reset_index()\n",
    "\n",
    "    # 将实际风险值合并到所有组合的数据框架中\n",
    "    result = pd.merge(df_all_combinations, risk_by_grid_and_time_slot, on=['grid_id', 'time_slot'], how='left')\n",
    "    \n",
    "    # 填充缺失值\n",
    "    result['risk_value'].fillna(0, inplace=True)\n",
    "\n",
    "    # 重命名列\n",
    "    result.columns = ['grid_id', 'time_slot', 'risk_value']\n",
    "\n",
    "    # 保存结果到CSV文件\n",
    "    result.to_csv(output_file, index=False)\n",
    "    print(f\"结果已保存到 {output_file}\")\n",
    "\n",
    "    \n",
    "process_collision_data(\n",
    "    collisions_file=r'dataset/Motor_Vehicle_Collisions_Crashes_2013.csv',\n",
    "    grids_file=r'save/NYC_grid_c.csv',\n",
    "    start_date='2013-01-01',\n",
    "    end_date='2014-01-01',\n",
    "    output_file='save/grid_risk_c.csv'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 调用函数进行处理\n",
    "# process_collision_data(\n",
    "#     collisions_file=r'dataset/Motor_Vehicle_Collisions_Crashes_2013.csv',\n",
    "#     grids_file=r'save/NYC_grid_c.csv',\n",
    "#     start_date='2013-01-01',\n",
    "#     end_date='2014-01-01',\n",
    "#     output_file='save/grid_risk_c.csv'\n",
    "# )\n",
    "# print('粗网格计算完成')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_collision_data(\n",
    "#     collisions_file=r'dataset/Motor_Vehicle_Collisions_Crashes_2013.csv',\n",
    "#     grids_file=r'save/NYC_grid_f.csv',\n",
    "#     start_date='2013-01-01',\n",
    "#     end_date='2014-01-01',\n",
    "#     output_file='save/grid_risk_f.csv'\n",
    "# )\n",
    "# print('细网格计算完成')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_collision_data(\n",
    "#     collisions_file=r'dataset/Motor_Vehicle_Collisions_Crashes_2013.csv',\n",
    "#     grids_file=r'save/NYC_grid_uf.csv',\n",
    "#     start_date='2013-01-01',\n",
    "#     end_date='2014-01-01',\n",
    "#     output_file='save/grid_risk_f.csv'\n",
    "# )\n",
    "# print('极细网格计算完成')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.生成天气\n",
    "import openmeteo_requests\n",
    "import requests_cache\n",
    "import pandas as pd\n",
    "from retry_requests import retry\n",
    "\n",
    "def fetch_weather_data(grid_file, output_file, start_date=\"2013-01-01\", end_date=\"2013-12-31\"):\n",
    "    \"\"\"\n",
    "    获取指定网格的天气数据并保存为CSV\n",
    "    \n",
    "    参数：\n",
    "    grid_file: str   网格数据文件路径\n",
    "    output_file: str 输出文件路径\n",
    "    start_date: str  数据开始日期 (默认2013-01-01)\n",
    "    end_date: str    数据结束日期 (默认2013-12-31)\n",
    "    \"\"\"\n",
    "    # 定义范围解析函数\n",
    "    def get_center(range_str):\n",
    "        \"\"\"从范围字符串计算中心点坐标\"\"\"\n",
    "        numbers = list(map(float, range_str.strip('()').split(', ')))\n",
    "        return sum(numbers) / 2\n",
    "\n",
    "    # 配置缓存和重试策略\n",
    "    cache_session = requests_cache.CachedSession('.cache', expire_after=-1)\n",
    "    retry_session = retry(cache_session, retries=5, backoff_factor=0.2)\n",
    "    openmeteo = openmeteo_requests.Client(session=retry_session)\n",
    "\n",
    "    # 读取并处理网格数据\n",
    "    grid_df = pd.read_csv(grid_file)\n",
    "    grid_df['latitude'] = grid_df['Latitude Range'].apply(get_center)\n",
    "    grid_df['longitude'] = grid_df['Longitude Range'].apply(get_center)\n",
    "\n",
    "    all_data = []\n",
    "\n",
    "    # 遍历每个网格点获取数据\n",
    "    for _, row in grid_df.iterrows():\n",
    "        grid_id = row['grid_id']\n",
    "        lat = row['latitude']\n",
    "        lon = row['longitude']\n",
    "        \n",
    "        print(f\"Processing Grid {grid_id} ({lat:.4f}, {lon:.4f})...\")\n",
    "        \n",
    "        # 配置API参数\n",
    "        url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "        params = {\n",
    "            \"latitude\": lat,\n",
    "            \"longitude\": lon,\n",
    "            \"start_date\": start_date,\n",
    "            \"end_date\": end_date,\n",
    "            \"hourly\": [\"precipitation\", \"weathercode\", \"windspeed_10m\"],\n",
    "            \"timezone\": \"America/New_York\"\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # 发送请求并解析响应\n",
    "            responses = openmeteo.weather_api(url, params=params)\n",
    "            response = responses[0]\n",
    "            \n",
    "            # 处理小时级数据\n",
    "            hourly = response.Hourly()\n",
    "            precipitation = hourly.Variables(0).ValuesAsNumpy()\n",
    "            weather_code = hourly.Variables(1).ValuesAsNumpy()\n",
    "            wind_speed = hourly.Variables(2).ValuesAsNumpy()\n",
    "            \n",
    "            # 生成时区转换后的时间序列\n",
    "            times = pd.date_range(\n",
    "                start=pd.to_datetime(hourly.Time(), unit=\"s\", utc=True),\n",
    "                end=pd.to_datetime(hourly.TimeEnd(), unit=\"s\", utc=True),\n",
    "                freq=pd.Timedelta(seconds=hourly.Interval()),\n",
    "                inclusive=\"left\"\n",
    "            ).tz_convert(\"America/New_York\")\n",
    "            \n",
    "            # 构建数据框\n",
    "            hourly_df = pd.DataFrame({\n",
    "                \"grid_id\": grid_id,\n",
    "                \"time_slot\": range(len(times)),\n",
    "                \"precipitation\": precipitation,\n",
    "                \"weather_code\": weather_code,\n",
    "                \"wind_speed_10m\": wind_speed\n",
    "            })\n",
    "            \n",
    "            all_data.append(hourly_df)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing grid {grid_id}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    # 合并并保存最终数据\n",
    "    if all_data:\n",
    "        final_df = pd.concat(all_data, ignore_index=True)\n",
    "        final_df.to_csv(output_file, index=False)\n",
    "        print(f\"数据已保存至 {output_file}\")\n",
    "    else:\n",
    "        print(\"未获取到有效数据\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Grid 0 (40.5345, -74.1372)...\n",
      "Processing Grid 1 (40.5261, -74.0172)...\n",
      "Processing Grid 2 (40.5223, -74.0547)...\n",
      "Processing Grid 3 (40.5590, -74.0055)...\n",
      "Processing Grid 4 (40.5590, -74.0649)...\n",
      "Processing Grid 5 (40.5590, -74.1167)...\n",
      "Processing Grid 6 (40.6010, -74.0056)...\n",
      "Processing Grid 7 (40.6010, -74.1167)...\n",
      "Processing Grid 8 (40.6010, -74.0611)...\n",
      "Processing Grid 9 (40.6346, -74.1167)...\n",
      "Processing Grid 10 (40.5248, -74.1723)...\n",
      "Processing Grid 11 (40.5180, -74.2275)...\n",
      "Processing Grid 12 (40.5554, -74.2231)...\n",
      "Processing Grid 13 (40.5590, -74.1723)...\n",
      "Processing Grid 14 (40.5856, -74.2020)...\n",
      "Processing Grid 15 (40.6010, -74.1723)...\n",
      "Processing Grid 16 (40.6255, -74.2006)...\n",
      "Processing Grid 17 (40.6316, -74.1723)...\n",
      "Processing Grid 18 (40.6429, -74.0056)...\n",
      "Processing Grid 19 (40.6429, -74.0611)...\n",
      "Processing Grid 20 (40.6818, -74.0402)...\n",
      "Processing Grid 21 (40.6849, -74.0056)...\n",
      "Processing Grid 22 (40.7268, -74.0014)...\n",
      "Processing Grid 23 (40.7687, -73.9970)...\n",
      "Processing Grid 24 (40.7963, -73.9827)...\n",
      "Processing Grid 25 (40.5735, -73.8470)...\n",
      "Processing Grid 26 (40.6010, -73.8332)...\n",
      "Processing Grid 27 (40.6022, -73.7469)...\n",
      "Processing Grid 28 (40.5990, -73.7833)...\n",
      "Processing Grid 29 (40.6494, -73.7833)...\n",
      "Processing Grid 30 (40.5672, -73.8945)...\n",
      "Processing Grid 31 (40.5660, -73.9500)...\n",
      "Processing Grid 32 (40.6010, -73.9002)...\n",
      "Processing Grid 33 (40.6010, -73.9500)...\n",
      "Processing Grid 34 (40.6429, -73.8945)...\n",
      "Processing Grid 35 (40.6429, -73.9500)...\n",
      "Processing Grid 36 (40.6849, -73.9500)...\n",
      "Processing Grid 37 (40.6849, -73.8945)...\n",
      "Processing Grid 38 (40.6429, -73.8389)...\n",
      "Processing Grid 39 (40.6495, -73.7403)...\n",
      "Processing Grid 40 (40.6849, -73.7833)...\n",
      "Processing Grid 41 (40.6849, -73.8389)...\n",
      "Processing Grid 42 (40.6849, -73.7406)...\n",
      "Processing Grid 43 (40.7268, -73.7833)...\n",
      "Processing Grid 44 (40.7268, -73.7278)...\n",
      "Processing Grid 45 (40.7687, -73.7833)...\n",
      "Processing Grid 46 (40.7652, -73.7283)...\n",
      "Processing Grid 47 (40.8099, -73.7918)...\n",
      "Processing Grid 48 (40.7268, -73.8389)...\n",
      "Processing Grid 49 (40.7268, -73.8945)...\n",
      "Processing Grid 50 (40.7268, -73.9500)...\n",
      "Processing Grid 51 (40.7687, -73.8945)...\n",
      "Processing Grid 52 (40.7687, -73.9500)...\n",
      "Processing Grid 53 (40.7687, -73.8389)...\n",
      "Processing Grid 54 (40.8107, -73.8945)...\n",
      "Processing Grid 55 (40.8107, -73.9500)...\n",
      "Processing Grid 56 (40.8107, -73.8389)...\n",
      "Processing Grid 57 (40.8526, -73.8389)...\n",
      "Processing Grid 58 (40.8556, -73.7899)...\n",
      "Processing Grid 59 (40.8807, -73.7984)...\n",
      "Processing Grid 60 (40.8526, -73.8945)...\n",
      "Processing Grid 61 (40.8526, -73.9377)...\n",
      "Processing Grid 62 (40.8920, -73.8389)...\n",
      "Processing Grid 63 (40.8944, -73.8945)...\n",
      "Processing Grid 64 (40.8772, -73.9260)...\n",
      "数据已保存至 save/grid_weather_c.csv\n"
     ]
    }
   ],
   "source": [
    "fetch_weather_data('save/NYC_grid_c.csv', 'save/grid_weather_c.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "11：00 0 0\n",
    "\n",
    "12：00 0 1.5\n",
    "\n",
    "13：00 3 2\n",
    "\n",
    "14：00 0 1.5\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
